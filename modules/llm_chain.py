import os
import re
from typing import List, Dict, Optional
from langchain.schema import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")
MODEL_NAME = os.getenv("GEMINI_MODEL_NAME", "gemini-pro")

# ë‹µë³€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
ANSWER_PROMPT = PromptTemplate(
    input_variables=["chat_history", "context", "question"],
    template="""
ë‹¹ì‹ ì€ í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ ì§€ì‹ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ì˜ì–´ ë…¼ë¬¸ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  í•™ìˆ ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

[ê·œì¹™]
1. ì œê³µëœ ë…¼ë¬¸ ë‚´ìš©ì—ì„œë§Œ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
2. ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.
3. ì£¼ì¥ ë˜ëŠ” ì£¼ìš” ì •ë³´ì—ëŠ” ë°˜ë“œì‹œ ì¸ìš© `({{citation}})` í˜•íƒœë¡œ í‘œì‹œí•˜ì„¸ìš”.
4. ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

{chat_history}

[ë…¼ë¬¸ ë‚´ìš©]
{context}

[ì§ˆë¬¸]
{question}
"""
)

# citation id ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_citation_ids(docs: List[Document]) -> List[str]:
    return list({doc.metadata.get("citation") for doc in docs if "citation" in doc.metadata})

# ë¡œì»¬ PDF ê²½ë¡œ ì¶”ì¶œ
def extract_pdf_paths(docs: List[Document]) -> List[str]:
    return list({doc.metadata.get("pdf_path") for doc in docs if "pdf_path" in doc.metadata})

# citation ì¹˜í™˜ ì²˜ë¦¬
def process_citations(text: str, citation_ids: List[str]) -> str:
    for citation_id in citation_ids:
        parts = citation_id.split("_")
        if len(parts) >= 2:
            author, year = parts[0], parts[1]
            formatted = f"{author} et al., {year}"
            text = text.replace(f"({{{{{citation_id}}}}})", f"({formatted})")
    return text

# context ê¸¸ì´ ì œí•œ
def truncate_context(docs: List[Document], max_chars: int = 6000) -> str:
    text = ""
    for doc in docs:
        chunk = f"[ì¶œì²˜: {doc.metadata.get('citation', 'Unknown')}]\n{doc.page_content}\n\n"
        if len(text) + len(chunk) > max_chars:
            break
        text += chunk
    return text

# ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ temperature ì„¤ì •
def detect_temperature(question: str) -> float:
    if re.search(r"(ì •ì˜|ëœ»|ë¬´ì—‡|ì˜ë¯¸)", question):
        return 0.0
    elif re.search(r"(ë¹„êµ|ì°¨ì´|ì¥ì |ë‹¨ì |íš¨ê³¼|ì˜ê²¬|ìƒê°)", question):
        return 0.5
    return 0.3

# ë©”ì¸ í´ë˜ìŠ¤
class LLMAnswerGenerator:
    def __init__(self, api_key: str = API_KEY, model_name: str = MODEL_NAME):
        self.llm = ChatGoogleGenerativeAI(
            google_api_key=api_key,
            model=model_name,
            temperature=0.3,
            max_output_tokens=1024
        )

    def estimate_k(self, question: str) -> int:
        prompt = f"""
ì§ˆë¬¸: "{question}"
ì´ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ë§ì€ ê´€ë ¨ ë…¼ë¬¸ ì¡°ê°(context)ì„ í™œìš©í•´ì•¼ ê°€ì¥ ì¢‹ì€ ë‹µì„ ë§Œë“¤ ìˆ˜ ìˆì„ì§€ íŒë‹¨í•´ì¤˜.
ê°€ëŠ¥í•œ ê°’ì€ 20, 30, 50 ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì§ˆë¬¸ì˜ ë³µì¡ì„±, ë¹„êµ ìš”êµ¬, ë…¼ìŸì„± ì—¬ë¶€ ë“±ì„ ê³ ë ¤í•´ ê²°ì •í•´.
ë‹µë³€ì€ ìˆ«ìë§Œ ì¶œë ¥í•´. ì˜ˆ: 30
"""
        try:
            response = self.llm.invoke(prompt)
            match = re.findall(r"\d+", response.content.strip())
            k = int(match[0]) if match else 20
            return k if k in [20, 30, 50] else 20
        except:
            return 20

    def generate_answer(
        self,
        question: str,
        docs: List[Document],
        context_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, any]:

        context_text = truncate_context(docs)
        citation_ids = extract_citation_ids(docs)
        pdf_paths = extract_pdf_paths(docs)
        temperature = detect_temperature(question)
        self.llm.temperature = temperature

        # ëŒ€í™” ë§¥ë½ êµ¬ì„±
        chat_history_text = ""
        if context_history:
            for turn in context_history:
                chat_history_text += f"\n[ì´ì „ ì§ˆë¬¸]\n{turn['user']}\n[ì´ì „ ë‹µë³€]\n{turn['assistant']}"

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = ANSWER_PROMPT.format(
            question=question,
            context=context_text,
            chat_history=chat_history_text
        )

        print(f"ğŸ§  Gemini í˜¸ì¶œ ì¤‘ (temperature={temperature})...")
        response = self.llm.invoke(prompt)
        answer = process_citations(response.content.strip(), citation_ids)

        return {
            "answer": answer,
            "citations": citation_ids,
            "pdf_paths": pdf_paths
        }

# ë©”íƒ€ ìš”ì²­ ì—¬ë¶€ íŒë‹¨
def is_meta_request(query: str) -> bool:
    return "ì œëª©" in query and ("ì§ˆë¬¸í•  ìˆ˜ ìˆëŠ”" in query or "ê°€ëŠ¥í•œ ë…¼ë¬¸" in query or "ë“±ë¡ëœ ë…¼ë¬¸" in query)
