import os
import re
from typing import List, Dict, Optional
from langchain.schema import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")
MODEL_NAME = os.getenv("GEMINI_MODEL_NAME", "gemini-pro")

# ë‹µë³€ í”„ë¡¬í”„íŠ¸
ANSWER_PROMPT = PromptTemplate(
    input_variables=["question", "context"],
    template="""
ë‹¹ì‹ ì€ í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ ì§€ì‹ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ì˜ì–´ ë…¼ë¬¸ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  í•™ìˆ ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

[ê·œì¹™]
1. ì œê³µëœ ë…¼ë¬¸ ë‚´ìš©ì—ì„œë§Œ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
2. ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.
3. ì£¼ì¥ ë˜ëŠ” ì£¼ìš” ì •ë³´ì—ëŠ” ë°˜ë“œì‹œ ì¸ìš© `({{citation}})` í˜•íƒœë¡œ í‘œì‹œí•˜ì„¸ìš”.
4. ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[ë…¼ë¬¸ ë‚´ìš©]
{context}

[ì§ˆë¬¸]
{question}
"""
)

# citation id ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_citation_ids(docs: List[Document]) -> List[str]:
    return list({doc.metadata.get("citation") for doc in docs if "citation" in doc.metadata})

# ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ
def extract_pdf_paths(docs: List[Document]) -> List[str]:
    return list({doc.metadata.get("pdf_path") for doc in docs if "pdf_path" in doc.metadata})

# citation ë¬¸ìì—´ ì¹˜í™˜
def process_citations(text: str, citation_ids: List[str]) -> str:
    for citation_id in citation_ids:
        parts = citation_id.split("_")
        if len(parts) >= 2:
            author, year = parts[0], parts[1]
            formatted = f"{author} et al., {year}"
            text = text.replace(f"({{{{citation_id}}}})", f"({formatted})")
    return text

# ì§ˆë¬¸ì— ë”°ë¼ Gemini ì˜¨ë„ ì„¤ì •
def detect_temperature(question: str) -> float:
    if re.search(r"(ì •ì˜|ëœ»|ë¬´ì—‡|ì–´ë–¤ ì˜ë¯¸)", question):
        return 0.0
    elif re.search(r"(ë¹„êµ|ì°¨ì´|ì¢‹ì€|íš¨ê³¼|ì¥ì |ë‹¨ì |ì˜ê²¬|ìƒê°)", question):
        return 0.5
    return 0.3

# ë©”ì¸ í´ë˜ìŠ¤
class LLMAnswerGenerator:
    def __init__(self, api_key=API_KEY, model_name=MODEL_NAME):
        self.llm = ChatGoogleGenerativeAI(
            google_api_key=api_key,
            model=model_name,
            temperature=0.3,
            max_output_tokens=1024
        )

    def estimate_k(self, question: str) -> int:
        prompt = f"""
ì§ˆë¬¸: "{question}"
ì´ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ë§ì€ ê´€ë ¨ ë…¼ë¬¸ ì¡°ê°(context)ì„ í™œìš©í•´ì•¼ ê°€ì¥ ì¢‹ì€ ë‹µì„ ë§Œë“¤ ìˆ˜ ìˆì„ì§€ íŒë‹¨í•´ì¤˜.
ê°€ëŠ¥í•œ ê°’ì€ 20, 30, 50 ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ ë§ì´ ì£¼ëŠ” ê²Œ ì•„ë‹ˆë¼ ì§ˆë¬¸ì´ êµ¬ì²´ì ì¸ì§€, ë³µì¡í•œ ë¹„êµë‚˜ ë…¼ìŸì„ ìš”êµ¬í•˜ëŠ”ì§€ë¥¼ ê³ ë ¤í•´ì„œ ê²°ì •í•´ì¤˜.
ë‹µë³€ í˜•ì‹ì€ ìˆ«ì í•˜ë‚˜ë§Œ ì¶œë ¥í•´ì¤˜. ì˜ˆ: 20
"""
        try:
            response = self.llm.invoke(prompt)
            parsed = int(re.findall(r"\\d+", response.content.strip())[0])
            return parsed if parsed in [20, 30, 50] else 20
        except:
            return 15

    def generate_answer(self, question: str, docs: List[Document], context_history: Optional[List[Dict[str, str]]] = None) -> Dict[str, any]:
        context_text = "\n\n".join([
            f"[ì¶œì²˜: {doc.metadata.get('citation', 'Unknown')}]\n{doc.page_content}"
            for doc in docs
        ])

        citation_ids = extract_citation_ids(docs)
        pdf_paths = extract_pdf_paths(docs)
        temperature = detect_temperature(question)
        self.llm.temperature = temperature

        # ëŒ€í™” ë§¥ë½ ë°˜ì˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        chat_history_text = ""
        if context_history:
            for turn in context_history:
                chat_history_text += f"\n[ì´ì „ ì§ˆë¬¸]\n{turn['user']}\n[ì´ì „ ë‹µë³€]\n{turn['assistant']}"

        prompt_text = f"""
{chat_history_text}

[ë…¼ë¬¸ ë‚´ìš©]
{context_text}

[ì§ˆë¬¸]
{question}
"""

        prompt = ANSWER_PROMPT.format(question=question, context=context_text)

        print(f"ğŸ§  Gemini í˜¸ì¶œ ì¤‘ (temperature={temperature})...")
        response = self.llm.invoke(prompt)
        answer = process_citations(response.content.strip(), citation_ids)

        return {
            "answer": answer,
            "citations": citation_ids,
            "pdf_paths": pdf_paths
        }

def is_meta_request(query: str) -> bool:
    return "ì œëª©" in query and ("ì§ˆë¬¸í•  ìˆ˜ ìˆëŠ”" in query or "ê°€ëŠ¥í•œ ë…¼ë¬¸" in query or "ë“±ë¡ëœ ë…¼ë¬¸" in query)