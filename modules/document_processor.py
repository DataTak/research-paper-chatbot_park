import os
import uuid
import json
from typing import List
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
import google.generativeai as genai

# Load .env settings
load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")
MODEL_NAME = os.getenv("GEMINI_MODEL_NAME", "gemini-2.0-flash")
if not API_KEY:
    raise ValueError("âŒ .env íŒŒì¼ì— GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


class MetadataExtractorLLM:
    """Gemini SDK ê¸°ë°˜ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° (ìµœëŒ€ 3í˜ì´ì§€ ì‹œë„, ì ì§„ì  ì •ë³´ ìˆ˜ì§‘)"""
    def __init__(self, model_name=MODEL_NAME, api_key=None):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(
            model_name,
            generation_config=genai.GenerationConfig(
                response_mime_type="application/json",
                response_schema={
                    "type": "object",
                    "properties": {
                        "title": {"type": "string"},
                        "authors": {"type": "string"},
                        "year": {"type": "string"},
                        "keywords": {"type": "string"},
                        "journal": {"type": "string"},
                        "abstract": {"type": "string"},
                        "doi": {"type": "string"},
                    },
                    "required": ["title", "authors", "year"]
                }
            )
        )

    def extract_metadata(self, pages: List[str]) -> dict:
        """1~3í˜ì´ì§€ ì¤‘ì—ì„œ ì•ìª½ ì •ë³´ ìš°ì„ , ì ì§„ì ìœ¼ë¡œ ë³´ì™„"""
        priority_keys = ["title", "authors", "year", "abstract"]
        optional_keys = ["keywords", "journal", "doi"]
        collected = {key: "" for key in priority_keys + optional_keys}

        for i, page_text in enumerate(pages[:3]):
            print(f"ğŸ” ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œë„ (í˜ì´ì§€ {i+1})...")
            try:
                response = self.model.generate_content(
                    f"""
    ë‹¤ìŒì€ ë…¼ë¬¸ ì¼ë¶€ì…ë‹ˆë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ê°€ëŠ¥í•œ í•œ ì •í™•íˆ ì¶”ì¶œí•´ì„œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ ì£¼ì„¸ìš”:

    - title
    - authors (ì‰¼í‘œë¡œ êµ¬ë¶„)
    - year
    - keywords (ì‰¼í‘œë¡œ êµ¬ë¶„)
    - journal
    - abstract
    - doi

    [ë…¼ë¬¸ í…ìŠ¤íŠ¸]
    {page_text}
    """, request_options={"timeout": 60})

                raw_json = response.text.strip()
                print("ğŸ“© Gemini ì‘ë‹µ (ì•ë¶€ë¶„):", raw_json[:100])
                parsed = json.loads(raw_json)

                for key in collected:
                    val = parsed.get(key, "").strip()
                    if not collected[key] and val.lower() not in ["", "unknown", "n.d.", "n/a"]:
                        collected[key] = val

                if all(collected[k] for k in priority_keys):
                    break

            except Exception as e:
                print(f"âš ï¸ LLM ì¶”ì¶œ ì‹¤íŒ¨ (í˜ì´ì§€ {i+1}): {e}")

        # í•„ìˆ˜ í•„ë“œ ë³´ì™„
        for key in priority_keys:
            if not collected[key]:
                collected[key] = "Unknown" if key != "year" else "n.d."

        return collected



class DocumentProcessor:
    def __init__(self, chunk_size=800, chunk_overlap=150):
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        self.extractor = MetadataExtractorLLM(api_key=API_KEY)

    def load_documents_from_folder(self, folder_path: str) -> List:
        all_chunks = []
        for root, _, files in os.walk(folder_path):
            for file in files:
                if file.endswith(".pdf"):
                    full_path = os.path.join(root, file)
                    chunks = self._process_single_file(full_path)
                    all_chunks.extend(chunks)
        return all_chunks

    def _process_single_file(self, filepath: str) -> List:
        loader = PyPDFLoader(filepath)
        raw_docs = loader.load()

        full_text = "\n".join([doc.page_content for doc in raw_docs])
        pages = [doc.page_content for doc in raw_docs[:3]]

        metadata = self.extractor.extract_metadata(pages)
        doc_id = str(uuid.uuid4())

        # â— ë©”íƒ€ë°ì´í„° ì´ìƒ ì—¬ë¶€ ë¡œê·¸ ê¸°ë¡
        if metadata["title"].lower() in ["", "unknown", "n/a"] or metadata["authors"].lower() in ["", "unknown", "n/a"]:
            print(f"âš ï¸ ë©”íƒ€ë°ì´í„° ì˜¤ë¥˜ - íŒŒì¼: {os.path.basename(filepath)}")
            print(f"    â®• title: {metadata['title']}")
            print(f"    â®• authors: {metadata['authors']}")
            os.makedirs("logs", exist_ok=True)
            with open("logs/metadata_failures.log", "a", encoding="utf-8") as f:
                f.write(f"{os.path.basename(filepath)}\n")

        chunks = self.splitter.create_documents([full_text])
        for i, chunk in enumerate(chunks):
            chunk.metadata = {
                "title": metadata.get("title"),
                "authors": metadata.get("authors"),
                "year": metadata.get("year"),
                "keywords": metadata.get("keywords"),
                "journal": metadata.get("journal"),
                "abstract": metadata.get("abstract"),
                "doi": metadata.get("doi"),
                "doc_id": doc_id,
                "citation": f'{metadata.get("authors").split(",")[0].strip()}_{metadata.get("year")}_Page_{i+1}',
                "pdf_path": filepath
            }

        return chunks