from modules.document_processor import DocumentProcessor

processor = DocumentProcessor()
chunks = processor.load_documents_from_folder("data/papers")

print(f"ì´ ì²­í¬ ìˆ˜: {len(chunks)}")

# ë…¼ë¬¸ë³„ë¡œ ì •ë¦¬ëœ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (doc_id ê¸°ì¤€ ê·¸ë£¹í•‘)
from collections import defaultdict
metadata_by_doc = defaultdict(list)

for chunk in chunks:
    metadata_by_doc[chunk.metadata["doc_id"]].append(chunk.metadata)

# ì¤‘ë³µ ì œê±°í•œ ë¬¸ì„œ ë‹¨ìœ„ ë©”íƒ€ë°ì´í„°ë§Œ ì¶œë ¥
print("\nğŸ“„ [ë…¼ë¬¸ë³„ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ê²°ê³¼ ìš”ì•½]\n")
for i, (doc_id, metas) in enumerate(metadata_by_doc.items(), start=1):
    meta = metas[0]  # ê°™ì€ ë…¼ë¬¸ì€ ë©”íƒ€ë°ì´í„°ê°€ ë™ì¼í•¨
    print(f"[{i}] ì œëª©: {meta['title']}")
    print(f"    ì €ì: {meta['authors']}")
    print(f"    ì—°ë„: {meta['year']}")
    print(f"    í‚¤ì›Œë“œ: {meta['keywords']}")
    print(f"    ì´ˆë¡: {meta['abstract'][:100]}...")  # ì• 100ìë§Œ ìš”ì•½
    print(f"    DOI: {meta['doi']}")
    print("-" * 80)
