import os
import streamlit as st
from modules.retriever import Retriever
from modules.llm_chain import LLMAnswerGenerator, is_meta_request
from modules.vector_store_builder import list_all_titles
from utils.s3_utils import S3Manager
from dotenv import load_dotenv

load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ğŸ“š í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ ì±—ë´‡", page_icon="ğŸ“˜")

# S3 ë§¤ë‹ˆì € ì´ˆê¸°í™” ë° DB ë‹¤ìš´ë¡œë“œ
def initialize_s3_and_db():
    if 's3_manager' not in st.session_state:
        st.session_state.s3_manager = S3Manager()
    
    db_path = "data/temp_vector_db/chroma.sqlite3"
    st.session_state.s3_manager.download_db_if_needed(db_path)
    return st.session_state.s3_manager, db_path

s3_manager, db_path = initialize_s3_and_db()

st.title("ğŸ“˜ ë°•ë°•ì‚¬ë‹˜ ê²½ì œë…¼ë¬¸ ì €ì¥ì†Œ ì±—ë´‡")
st.markdown("ì˜ì–´ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ì¶œì²˜ê°€ ë‹¬ë¦° ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\në“±ë¡ëœ ë…¼ë¬¸ë“¤ì´ ê¶ê¸ˆí•˜ë©´ \"ë“±ë¡ëœ ë…¼ë¬¸ ì œëª© ì•Œë ¤ì¤˜\" ë¼ê³  ë¬¼ì–´ë³´ì„¸ìš”.")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Retriever ë° LLM ì´ˆê¸°í™”
retriever = Retriever()
llm = LLMAnswerGenerator()

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´")
    st.markdown("- ê²€ìƒ‰ ëª¨ë¸: `BAAI/bge-m3`")
    st.markdown("- ë‹µë³€ ëª¨ë¸: `Gemini-2.0-flash`")
    st.markdown("- ì €ì¥ì†Œ: `AWS S3 + Chroma`")
    
    if st.button("ğŸ’¾ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.chat_history = []
        
    if st.button("ï¿½ï¿½ DB ìƒˆë¡œê³ ì¹¨"):
        st.session_state.s3_manager.download_db_if_needed(db_path, force_download=True)
        st.success("DBê°€ ìƒˆë¡œê³ ì¹¨ë˜ì—ˆìŠµë‹ˆë‹¤!")

# ê¸°ì¡´ ëŒ€í™” í‘œì‹œ
for entry in st.session_state.chat_history:
    with st.chat_message(entry["role"]):
        st.markdown(entry["content"])

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
query = st.chat_input("ë…¼ë¬¸ì—ì„œ ì–´ë–¤ ë¶€ë¶„ì´ ê¶ê¸ˆí•œê°€ìš”?")
if query:
    st.session_state.chat_history.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    with st.chat_message("assistant"):
        with st.spinner("ğŸ” ê´€ë ¨ ë…¼ë¬¸ì„ ì°¾ê³ , í•™ìˆ ì ìœ¼ë¡œ ë‹µë³€ ì¤‘ì…ë‹ˆë‹¤..."):
            if is_meta_request(query):
                titles = list_all_titles("data/vector_db/")
                answer = "### ğŸ“š í˜„ì¬ ë“±ë¡ëœ ë…¼ë¬¸ ëª©ë¡:\n" + "\n".join([f"- {title}" for title in titles])
            else:
                # ëŒ€í™” ë§¥ë½ ìˆ˜ì§‘
                context_history = [
                    {"user": h["content"], "assistant": st.session_state.chat_history[i+1]["content"]}
                    for i, h in enumerate(st.session_state.chat_history[:-1]) if h["role"] == "user"
                ]

                # ë“±ë¡ëœ ì œëª©ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš° ì „ì²´ ë…¼ë¬¸ìœ¼ë¡œ ì¶”ì¶œ
                all_titles = list_all_titles("data/vector_db/")
                matched_title = next((t for t in all_titles if t.strip('"').lower() in query.lower()), None)

                if matched_title:
                    st.info(f"ğŸ“„ ë…¼ë¬¸ ì „ì²´ë¡œ ë¶„ì„ ì¤‘: {matched_title}")
                    docs = retriever.query_documents_by_title(matched_title.strip('"'))
                else:
                    # ì¼ë°˜ ê²€ìƒ‰ + k ì¶”ì •
                    k = llm.estimate_k(query)
                    docs = retriever.query_similar_documents(query, k=k)

                result = llm.generate_answer(query, docs, context_history=context_history)
                answer = result["answer"]

        st.markdown(answer)
        st.session_state.chat_history.append({"role": "assistant", "content": answer})